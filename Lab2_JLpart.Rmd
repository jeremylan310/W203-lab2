---
title: "Lab 2: What Makes a Product Successful? Fall 2021"
author: 'w203: Statistics for Data Science'
date: "November 1, 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r echo=FALSE, warning=FALSE, message=FALSE}
library(readr)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(lmtest)

# read data as is
fb_data = read_delim("dataset_Facebook.csv", delim=";")
# change col names
cols = c(
  "page_ttl_likes",
  "type",
  "category",
  "post_month",
  "post_wkday",
  "post_hour",
  "paid",
  "lifetime_post_reach",
  "lifetime_post_impressions",
  "lifetime_engaged_users",
  "lifetime_post_consumers",
  "lifetime_post_consumptions",
  "lifetime_post_impressions_from_page_have_likeds",
  "lifetime_post_impressions_from_page_likes",
  "lifetime_people_liked_engaged_with_post",
  "comments",
  "likes",
  "shares",
  "interactions"
)
colnames(fb_data) = cols

fb_data = fb_data %>%
  mutate(
    category_str = case_when(
      category == 1 ~ "Action",
      category == 2 ~ "Product",
      category == 3 ~ "Inspiration"
    ),
    period_of_day=case_when(
      post_hour >= 1  & post_hour <= 5  ~ "Overnight",
      post_hour >= 6  & post_hour <= 11 ~ "Morning",
      post_hour >= 12 & post_hour <= 17 ~ "Afternoon",
      post_hour >= 18 & post_hour <= 23 ~ "Evening"
    ),
    weekend=case_when(
      post_wkday == 1 | post_wkday == 7 ~ 1,
      TRUE                              ~ 0
    )
  ) %>% filter(!is.na(paid))
# split data
set.seed(1)
split = sample(c(rep(0, 0.7 * nrow(fb_data)), rep(1, 0.3 * nrow(fb_data))))
split = append(split, 1)
fb_data_exp = fb_data[split==1,]
fb_data_test = fb_data[split==0,]


inter_model = lm(log(lifetime_engaged_users) ~ 1 + 
                  type*category_str + 
                  paid + 
                  period_of_day + weekend 
                , data=fb_data_test)
```
### 5. Limitations of your Model 

#### 5a. Large-Sample Assumptions \newline
The sample size of 500 observations indicates that we must evaluate and consider the following large-sample assumptions as pertaining to our model.

1. Data is Independent and Identically Distributed (I.I.D)

It is evident that our posts dataset is not I.I.D. As all of the posts are being made by the same cosmetics company and on the same platform (Facebook), inevitably all posts will be linked to the same company's products and promotions. Although we do not have unique detail regarding each post, it's likely that multiple posts may be tied to the same product or promotion. Other post parameters (time/date) may also not be independent, depending on the marketing strategy the team employed at the time this study was performed. Lastly, the number of impressions on each post are not independent from one another as many of the engagements/interactions may be from the same followers of the page across multiple posts. 

Unfortunately, there is not much we can do to improve the raw data itself to reduce I.I.D. concerns. As all of the posts are anonymous we can't directly discern how to cluster or aggregate datapoints. Using robust standard errors may mitigate non-I.I.D. concerns and account for any heteroscedasticity in our model, although this is not an ideal solution. A Breusch-Pagan homoscedasticity test was run on our model - the high p-value above .05 means that we *fail* to reject $H_0$, meaning that we can't find enough evidence to say that homoscedasticity is not present. 

```{r}
bptest(inter_model)
```

2. Unique BLP Exists - No perfect collinearity

To satisfy this assumption, no variable in our model can be written as a linear combination of the other variables in the model. This assumption is satisfied given the following correlation matrix (one-hot encoding of the *type* and *category* variables was done to test correlation):
```{r echo=FALSE, warning=FALSE, message=FALSE}
install.packages('corrplot')
library(corrplot)

fb_data = fb_data %>% 
  mutate(type_Photo = ifelse(type == "Photo", 1, 0),
  type_Status = ifelse(type == "Status", 1, 0),
  type_Video = ifelse(type == "Video", 1, 0),
  type_Link = ifelse(type == "Link", 1, 0))

fb_data = fb_data %>% 
  mutate(cat_Action = ifelse(category == 1, 1, 0),
  cat_Product = ifelse(category == 2, 1, 0),
  cat_Inspiration = ifelse(category == 3, 1, 0))

corrplot(cor(fb_data[c("lifetime_engaged_users", "post_wkday", "post_hour", "cat_Action", "cat_Product", "cat_Inspiration", "type_Photo", "type_Status", "type_Video", "type_Link")]),method="color")
```

#### 5b. Omitted Variables \newline \newline

**Intentionally omitted variables** 

Some variables were accessible within the dataset but were omitted, including:

- Total Page Likes
- Lifetime post total reach
- Lifetime post total impressions
- Lifetime post impressions by people who have liked the page
- Lifetime post reach by people who have liked the page
- Lifetime people who have liked page and engaged with post
- Comments
- Likes
- Shares

As our outcome variable *engaged users* is defined as the number of users who clicked on the post in question, we can see that many of the intentionally omitted variables are very similar in what they measure - most are defined as some metric of engagement. As we determined that *engaged users* would be the best-suited outcome variable for our model, as it was a more holistic summation of engagement than individual metrics such as *comments* or *post reach only by people who liked the page*. 

These variables that could be classified as outcome variables were omitted, as including these in the model would obscure the effects and relationships of the true measured variables. For example, we could have included "likes" as a covariate and examined its effect on user engagement. However, "likes" and engagement are directly related, and including this parameter in our model would absorb causal effect from other variables that we are trying to measure. 

**Unintentionally omitted variables**

Many other variables that may affect user engagement were not tracked in this dataset, as a result they are not able to be factored into our model. We would like to address some of these omitted variables below and discuss their potential impact on the model results. 

- Desirability of the product being marketed: 

Not all products that the company promotes on its page will be of equal desirability to its customers. For example, two posts of the same type and category (not to mention all of the other supporting covariates) could have wildly varying engagement figures depending on if the product was highly in-demand or not. If we had knowledge of the product/service being promoted in each post, we would be able to account for it in our model by factoring in parameters such as the sales figures of each product, market sentiment of each product, etc. (this information could likely be obtained from external sources). We would then be able to add sales figures or another product desirability metric into our model; promoting a more well-known product would likely lead to more engagement than a more obscure product, all other factors constant. 

- Quality of the promotional material: 

Similarly, not all posts made by the company on the Facebook page will be of similar quality. Even if posts are classified here as "Photo" or "Video", posting a well-designed infographic may lead to more engagement than sharing an image of lower design quality. This may be hard to objectively quantify but if we had access to the contents of each post it's possible that we would be able to classify posts as "high quality" or "low quality". Incorporating this variable into the model may help the company decide how much to invest in its graphic design/marketing agencies - if there is a significant effect when a higher-quality post is used, we may be able to quantify the positive effect a higher-quality post has on user engagement and product revenue.

Both of these omitted variables ultimately affect the brand awareness of the company, as more desirable products and higher quality content both would increase traffic to the site. If these omitted variables were included in our analysis, potentially we could engineer a more holistic brand awareness metric as an input into our model. Both of these variables directly have an effect on user engagement - the full causal graph of our model is shown below. 


```{r, echo=FALSE, out.width="75%", fig.cap="Causal relationship graph", fig.align = "center"}
knitr::include_graphics("Lab2Causal_2.jpg")
```

In summary, one of the main limitations of this dataset is the amount of information regarding the posts themselves - although we know the type and category of post, we don't have visibility to the nature of the post contents, which we assume can have an effect on the company's brand awareness as well as user engagement. 